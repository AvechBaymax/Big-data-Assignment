# ğŸšŒ Bus GPS Data Pipeline

Dá»± Ã¡n **Big Data Pipeline** xá»­ lÃ½ dá»¯ liá»‡u hÃ nh trÃ¬nh xe buÃ½t (GPS) theo **Real-time Streaming** vÃ  **Batch Processing**, tÃ­ch há»£p cÃ¡c cÃ´ng nghá»‡: **Kafka, Hadoop HDFS, Spark, Hive vÃ  Trino**.

![Architecture](https://img.shields.io/badge/Architecture-Kafka%20%7C%20Spark%20%7C%20HDFS%20%7C%20Trino-blue)
![Status](https://img.shields.io/badge/Status-Completed-success)

---

## ğŸ“Š Dataset

* **Nguá»“n dá»¯ liá»‡u**: Giáº£ láº­p GPS xe buÃ½t (~1.000.000 báº£n ghi)
* **Thuá»™c tÃ­nh**:

  * `datetime`
  * `vehicle_id`
  * `lng`, `lat`
  * `speed`
  * `driver`
  * `door_status`
* **Luá»“ng dá»¯ liá»‡u**:

```
CSV Raw â†’ Kafka (Streaming) â†’ Spark (Processing) â†’ HDFS (Storage) â†’ Trino (Analytics)
```

---

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    CSV[CSV Raw Data] -->|Python Producer| Kafka[Apache Kafka]
    Kafka -->|Streaming| Console[Console Consumer]

    CSV -->|Upload| HDFS_Raw[HDFS /data/raw]
    HDFS_Raw -->|Batch Process| Spark[Apache Spark]
    Spark -->|Aggregation| HDFS_Processed[HDFS /data/processed]

    Postgres[PostgreSQL] -->|Metadata| Hive[Hive Metastore]
    HDFS_Processed -->|Query Data| Trino[Trino Query Engine]
    Hive -->|Schema| Trino
```

---

## ğŸ³ Tech Stack

| Component  | Technology     | Version | Role                                |
| ---------- | -------------- | ------- | ----------------------------------- |
| Ingestion  | Apache Kafka   | 7.4.0   | Message Queue cho dá»¯ liá»‡u Realâ€‘time |
| Storage    | Hadoop HDFS    | 3.2.1   | LÆ°u trá»¯ dá»¯ liá»‡u phÃ¢n tÃ¡n            |
| Processing | Apache Spark   | 3.4.0   | Batch processing & aggregation      |
| Metadata   | Hive Metastore | 2.3.2   | Quáº£n lÃ½ schema (PostgreSQL backend) |
| Analytics  | Trino          | 427     | SQL query trÃªn HDFS                 |
| Infra      | Docker Compose | â€“       | Quáº£n lÃ½ háº¡ táº§ng container           |

---

## ğŸš€ Quick Start Guide

### 1. Khá»Ÿi táº¡o mÃ´i trÆ°á»ng

```bash
# Clone repo & setup Python virtual environment
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Khá»Ÿi Ä‘á»™ng toÃ n bá»™ há»‡ thá»‘ng (9 services)
docker compose up -d
```

---

### 2. Cáº¥u hÃ¬nh Hive Metastore (Quan trá»ng)

Hive Metastore dÃ¹ng PostgreSQL backend, cáº§n init schema **chá»‰ má»™t láº§n**:

```bash
# Khá»Ÿi táº¡o schema cho Hive
docker compose run --rm hive-metastore \
  /opt/hive/bin/schematool -dbType postgres -initSchema

# Restart Hive Metastore
docker restart hive-metastore
```

---

### 3. Táº¡o dá»¯ liá»‡u máº«u

```bash
# Táº¡o file giáº£ láº­p ~1 triá»‡u báº£n ghi
python3 scripts/create_sample_data.py
# Chá»n: sample_medium_test.csv
```

---

## ğŸ§ª Testing Scenarios

### Scenario 1: Real-time Streaming (Kafka)

**Consumer (Terminal 1):**

```bash
docker exec big-data-assignment-kafka-1 kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic bus-gps-tracking \
  --from-beginning
```

**Producer (Terminal 2):**

```bash
python3 src/kafka/producer.py data/samples/sample_medium_test.csv
```

âœ… **Ká»³ vá»ng**: Consumer hiá»ƒn thá»‹ dá»¯ liá»‡u JSON liÃªn tá»¥c.

---

### Scenario 2: Batch Processing (Spark & HDFS)

**Upload dá»¯ liá»‡u lÃªn HDFS:**

```bash
docker exec big-data-assignment-namenode-1 \
  hdfs dfs -mkdir -p /data/raw

docker cp data/raw_2025-04-01.csv \
  big-data-assignment-namenode-1:/tmp/

docker exec big-data-assignment-namenode-1 \
  hdfs dfs -put -f /tmp/raw_2025-04-01.csv /data/raw/
```

**Submit Spark Job:**

```bash
docker cp src/spark/batch_processing.py \
  big-data-assignment-spark-master-1:/opt/spark/work-dir/

docker exec big-data-assignment-spark-master-1 \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark/work-dir/batch_processing.py
```

âœ… **Ká»³ vá»ng**: Log `Batch Job Completed Successfully!`

**Kiá»ƒm tra HDFS:**

```bash
docker exec big-data-assignment-namenode-1 \
  hdfs dfs -ls -R /data/processed
```

---

### Scenario 3: Data Analytics (Trino SQL)

**VÃ o Trino CLI:**

```bash
docker exec -it trino trino
```

**Táº¡o schema & báº£ng:**

```sql
CREATE SCHEMA IF NOT EXISTS hive.bus_data;

CREATE TABLE IF NOT EXISTS hive.bus_data.driver_stats (
   driver varchar,
   avg_speed double,
   max_speed double,
   trip_count bigint
)
WITH (
   format = 'PARQUET',
   external_location = 'hdfs://namenode:9000/data/processed/driver_stats'
);
```

**Query phÃ¢n tÃ­ch:**

```sql
SELECT *
FROM hive.bus_data.driver_stats
ORDER BY trip_count DESC
LIMIT 10;
```

âœ… **Ká»³ vá»ng**: `trip_count > 1000`

---

## ğŸ› ï¸ Troubleshooting & Fixes

### Python 3.12 Compatibility

* **Lá»—i**: `ModuleNotFoundError: No module named 'distutils'`
* **Fix**: `numpy>=1.26.4`, `setuptools` trong `requirements.txt`

### Trino HDFS Connection

* **Lá»—i**: `External location is not a valid file system URI`
* **Fix**:

  * Downgrade `trinodb/trino:427`
  * Cáº¥u hÃ¬nh `hive.config.resources` â†’ `core-site.xml`

### CSV / Encoding Issues

* **Lá»—i**: KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file CSV (BOM, delimiter `\t`)
* **Fix**:

  * Python: `encoding='utf-8-sig'`
  * Spark: `delimiter="\t"`

---

## ğŸ“‹ Project Status

* [x] Docker Infrastructure (9 services healthy)
* [x] Data Generation
* [x] Kafka Streaming
* [x] HDFS Storage
* [x] Spark Batch Processing
* [x] Hive Metastore Integration
* [x] Trino Analytics

---

## ğŸ“„ License

MIT License
